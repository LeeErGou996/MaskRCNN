{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/LeeErGou996/MaskRCNN.git\n",
    "!pip install torch torchvision matplotlib Pillow tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc245801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/MaskRCNN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f190f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing directory: ./train\n",
      "Removing existing directory: ./val\n",
      "Removing existing directory: ./test\n",
      "Created fresh dataset directories.\n",
      "Found 8 images and 8 masks in source directory\n",
      "Class distribution in the dataset:\n",
      " - Class BAS: 1 images\n",
      " - Class EBO: 1 images\n",
      " - Class EOS: 1 images\n",
      " - Class LYT: 1 images\n",
      " - Class MON: 1 images\n",
      " - Class MYO: 1 images\n",
      " - Class NGB: 1 images\n",
      " - Class NGS: 1 images\n",
      "Warning: Not enough data for validation set. Requested 4, but only got 0.\n",
      "Warning: Not enough data for test set. Requested 4, but only got 0.\n",
      "\n",
      "Final class distribution in training set:\n",
      " - Class BAS: 1 images\n",
      " - Class EBO: 1 images\n",
      " - Class EOS: 1 images\n",
      " - Class LYT: 1 images\n",
      " - Class MON: 1 images\n",
      " - Class MYO: 1 images\n",
      " - Class NGB: 1 images\n",
      " - Class NGS: 1 images\n",
      "\n",
      "Final class distribution in validation set:\n",
      "\n",
      "Final class distribution in test set:\n",
      "\n",
      "Dataset split complete:\n",
      " - Training set: 8/8 images copied\n",
      " - Validation set: 0/0 images copied\n",
      " - Test set: 0/0 images copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\miniconda3\\envs\\torch2_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset sizes: Train=8, Val=0, Test=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/1 [00:03<?, ?it/s]        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 770\u001b[0m\n\u001b[0;32m    758\u001b[0m stats \u001b[38;5;241m=\u001b[39m initialize_dataset(\n\u001b[0;32m    759\u001b[0m     data_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSPenn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSPenn\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Path to the original dataset\u001b[39;00m\n\u001b[0;32m    760\u001b[0m     train_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./train\u001b[39m\u001b[38;5;124m\"\u001b[39m,                  \u001b[38;5;66;03m# Path for training data\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    766\u001b[0m     min_images_per_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Ensure at least 2 images per class in each set\u001b[39;00m\n\u001b[0;32m    767\u001b[0m )\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 770\u001b[0m model, best_metric, val_metrics, test_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    776\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;66;03m# Print summary of results\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 236\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_dir, val_dir, test_dir, output_dir, num_epochs)\u001b[0m\n\u001b[0;32m    233\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Forward pass and compute loss\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\miniconda3\\envs\\torch2_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\miniconda3\\envs\\torch2_env\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     94\u001b[0m             degen_bb: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[bb_idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     95\u001b[0m             torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[0;32m     96\u001b[0m                 \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll bounding boxes should have positive height and width.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found invalid box \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegen_bb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for target at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m             )\n\u001b[1;32m--> 101\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\miniconda3\\envs\\torch2_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\DATA\\桌面\\InDeutschland\\myMaskRCNN\\mymaskrcnn.py:195\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m    194\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody(x)\n\u001b[1;32m--> 195\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\miniconda3\\envs\\torch2_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\DATA\\桌面\\InDeutschland\\myMaskRCNN\\mymaskrcnn.py:130\u001b[0m, in \u001b[0;36mCustomFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# 当前层的lateral connection\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     inner_lateral \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_blocks[idx](x[names[idx]])\n\u001b[1;32m--> 130\u001b[0m     inner_lateral \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_lateral\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 应用dropout\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# 上采样和特征融合\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     feat_shape \u001b[38;5;241m=\u001b[39m inner_lateral\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\miniconda3\\envs\\torch2_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\DATA\\桌面\\InDeutschland\\myMaskRCNN\\dropblock.py:100\u001b[0m, in \u001b[0;36mAlwaysDropBlock2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     97\u001b[0m mask \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrand(batch_size, channels, height, width, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m<\u001b[39m gamma)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# 扩展掩码为 block_size 的连续块\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mFF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask  \u001b[38;5;66;03m# 反转掩码，丢弃区域为 0\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# 重新缩放输入\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\miniconda3\\envs\\torch2_env\\lib\\site-packages\\torch\\_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\miniconda3\\envs\\torch2_env\\lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor, MaskRCNN_ResNet50_FPN_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom modules (assuming these are available in your environment)\n",
    "# You'll need to implement or obtain these modules separately\n",
    "from detection.engine import train_one_epoch, evaluate\n",
    "import detection.utils as utils\n",
    "import detection.transforms as T\n",
    "from mymaskrcnn import custom_maskrcnn_resnet50_fpn\n",
    "\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Define class mapping\n",
    "        self.CLASSES = {\n",
    "            'EOS': 1,\n",
    "            'LYT': 2,\n",
    "            'MON': 3,\n",
    "            'MYO': 4,\n",
    "            'NGB': 5,\n",
    "            'NGS': 6,\n",
    "            'EBO': 7,\n",
    "            'BAS': 8\n",
    "        }  # Background is 0, target classes start from 1\n",
    "\n",
    "        # Match image and mask files\n",
    "        self.imgs = list(sorted([f for f in os.listdir(os.path.join(root, \"PNGImages\")) if f.endswith(('.png', '.jpg'))]))\n",
    "        self.masks = list(sorted([f for f in os.listdir(os.path.join(root, \"PedMasks\")) if f.endswith(('.png', '.jpg'))]))\n",
    "\n",
    "        if len(self.imgs) != len(self.masks):\n",
    "            raise ValueError(f\"Number of images ({len(self.imgs)}) and masks ({len(self.masks)}) do not match!\")\n",
    "\n",
    "        for img, mask in zip(self.imgs, self.masks):\n",
    "            if os.path.splitext(img)[0] != os.path.splitext(mask)[0]:\n",
    "                raise ValueError(f\"Mismatch between image and mask: {img} and {mask}\")\n",
    "\n",
    "    def get_class_from_filename(self, filename):\n",
    "        class_name = filename.split('_')[0]\n",
    "        if class_name not in self.CLASSES:\n",
    "            print(f\"Warning: Class name '{class_name}' not found in CLASSES. Defaulting to background (0).\")\n",
    "        return self.CLASSES.get(class_name, 0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.imgs[idx]\n",
    "        mask_name = self.masks[idx]\n",
    "\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", img_name)\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", mask_name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        # Get class label\n",
    "        class_id = self.get_class_from_filename(img_name)\n",
    "\n",
    "        # Combine all masks into one\n",
    "        combined_mask = (mask > 0).astype(np.uint8)\n",
    "        pos = np.where(combined_mask)\n",
    "\n",
    "        if pos[0].size == 0 or pos[1].size == 0:\n",
    "            raise ValueError(f\"Invalid mask at index {idx}\")\n",
    "\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "\n",
    "        # Create bounding box and mask\n",
    "        boxes = torch.tensor([[xmin, ymin, xmax, ymax]], dtype=torch.float32)\n",
    "        masks = torch.as_tensor(combined_mask, dtype=torch.uint8).unsqueeze(0)\n",
    "        labels = torch.tensor([class_id], dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "            \"iscrowd\": torch.zeros((1,), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # Load a pre-trained model\n",
    "    model = custom_maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    \n",
    "    # Replace the classifier with a new one for num_classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # Replace the mask predictor with a new one for num_classes\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = [T.ToTensor()]\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def train_model(train_dir, val_dir, test_dir, output_dir, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train the Mask R-CNN model and evaluate on validation and test sets.\n",
    "    Added tqdm progress bars for better progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        train_dir: Directory containing training data\n",
    "        val_dir: Directory containing validation data\n",
    "        test_dir: Directory containing test data\n",
    "        output_dir: Directory to save model and results\n",
    "        num_epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    # Import tqdm\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Number of classes (background + 8 object classes)\n",
    "    num_classes = 9\n",
    "\n",
    "    # Load datasets\n",
    "    dataset = PennFudanDataset(train_dir, get_transform(train=True))\n",
    "    dataset_val = PennFudanDataset(val_dir, get_transform(train=False))\n",
    "    dataset_test = PennFudanDataset(test_dir, get_transform(train=False))\n",
    "\n",
    "    print(f\"Dataset sizes: Train={len(dataset)}, Val={len(dataset_val)}, Test={len(dataset_test)}\")\n",
    "\n",
    "    # Create data loaders with 0 workers to avoid multiprocessing issues\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn\n",
    "    )\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=2, shuffle=False, num_workers=0, collate_fn=utils.collate_fn\n",
    "    )\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=2, shuffle=False, num_workers=0, collate_fn=utils.collate_fn\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0008)\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    import math\n",
    "    def warmup_lr_lambda(current_step):\n",
    "        warmup_steps = 3  # Number of warmup epochs\n",
    "        if current_step < warmup_steps:\n",
    "            return 0.1 + (1.0 - 0.1) * current_step / warmup_steps\n",
    "        else:\n",
    "            # Cosine annealing after warmup\n",
    "            decay_steps = num_epochs - warmup_steps\n",
    "            cosine_decay = 0.5 * (1 + math.cos(math.pi * (current_step - warmup_steps) / decay_steps))\n",
    "            return max(0.1, cosine_decay)  # Lower limit is 10% of initial LR\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr_lambda)\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    best_eval_metric = float('-inf')\n",
    "    train_loss = []\n",
    "    learning_rates = []\n",
    "    val_maps = []\n",
    "    val_maps_75 = []\n",
    "    val_maps_all = []\n",
    "    \n",
    "    # Create log files\n",
    "    now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    det_results_file = os.path.join(output_dir, f\"det_results_{now}.txt\")\n",
    "    seg_results_file = os.path.join(output_dir, f\"seg_results_{now}.txt\")\n",
    "    test_log_file = os.path.join(output_dir, f\"test_log_{now}.txt\")\n",
    "    metrics_summary_file = os.path.join(output_dir, f\"metrics_summary.txt\")\n",
    "    train_log_file = os.path.join(output_dir, f\"train_log_{now}.txt\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Progress bar for epochs\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training Progress\", position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        # Update progress bar description\n",
    "        epoch_pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        \n",
    "        # Modified training loop with progress bar\n",
    "        train_loss_epoch = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Progress bar for batches\n",
    "        batch_pbar = tqdm(data_loader, desc=f\"Training Batch\", position=1, leave=False)\n",
    "        \n",
    "        for images, targets in batch_pbar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass and compute loss\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update batch progress bar\n",
    "            batch_pbar.set_postfix(loss=f\"{losses.item():.4f}\")\n",
    "            \n",
    "            # Accumulate loss\n",
    "            train_loss_epoch += losses.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        mean_loss = train_loss_epoch / batch_count if batch_count > 0 else 0\n",
    "        train_loss.append(mean_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        learning_rates.append(lr)\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix(loss=f\"{mean_loss:.4f}\", lr=f\"{lr:.6f}\")\n",
    "\n",
    "    # In the train_model function, modify the validation part:\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "\n",
    "    # Progress bar for validation\n",
    "    val_pbar = tqdm(data_loader_val, desc=f\"Validating\", position=1, leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Perform COCO evaluation\n",
    "        eval_result_val = evaluate(model, data_loader_val, device=device)\n",
    "        \n",
    "        # Extract validation metrics with error handling\n",
    "        try:\n",
    "            # Try to get both bbox and segm metrics\n",
    "            val_det_metric = eval_result_val.coco_eval['bbox'].stats[1]  # AP50\n",
    "            val_seg_metric = eval_result_val.coco_eval['segm'].stats[1] if 'segm' in eval_result_val.coco_eval else 0.0\n",
    "            \n",
    "            val_det_metric_75 = eval_result_val.coco_eval['bbox'].stats[2]  # AP75\n",
    "            val_seg_metric_75 = eval_result_val.coco_eval['segm'].stats[2] if 'segm' in eval_result_val.coco_eval else 0.0\n",
    "            \n",
    "            val_det_metric_all = eval_result_val.coco_eval['bbox'].stats[0]  # AP0.5:0.95\n",
    "            val_seg_metric_all = eval_result_val.coco_eval['segm'].stats[0] if 'segm' in eval_result_val.coco_eval else 0.0\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Warning: Could not extract some metrics from validation evaluation: {e}\")\n",
    "            # Provide default values for missing metrics\n",
    "            if 'bbox' not in eval_result_val.coco_eval:\n",
    "                val_det_metric = val_det_metric_75 = val_det_metric_all = 0.0\n",
    "                print(\"Detection metrics missing\")\n",
    "            if 'segm' not in eval_result_val.coco_eval:\n",
    "                val_seg_metric = val_seg_metric_75 = val_seg_metric_all = 0.0\n",
    "                print(\"Segmentation metrics missing - this may be normal if model doesn't predict masks\")\n",
    "        \n",
    "        # Compute combined metric\n",
    "        current_metric = (val_det_metric + val_seg_metric) / 2\n",
    "        val_maps.append(current_metric)\n",
    "        val_maps_75.append((val_det_metric_75 + val_seg_metric_75) / 2)\n",
    "        val_maps_all.append((val_det_metric_all + val_seg_metric_all) / 2)\n",
    "        \n",
    "        # Update epoch progress bar with validation metrics\n",
    "        epoch_pbar.set_postfix(loss=f\"{mean_loss:.4f}\", val_mAP50=f\"{current_metric:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if current_metric > best_eval_metric:\n",
    "            best_eval_metric = current_metric\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, \"best_model.pth\"))\n",
    "            print(f\"\\nSaved best model with mAP50: {best_eval_metric:.4f}\")\n",
    "\n",
    "        # Log training and validation info\n",
    "        with open(train_log_file, \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch + 1}: Train Loss: {mean_loss:.4f}, \"\n",
    "                   f\"Val Det mAP50: {val_det_metric:.4f}, Val Seg mAP50: {val_seg_metric:.4f}, \"\n",
    "                   f\"Val Det mAP75: {val_det_metric_75:.4f}, Val Seg mAP75: {val_seg_metric_75:.4f}, \"\n",
    "                   f\"Val Det mAP0.5:0.95: {val_det_metric_all:.4f}, Val Seg mAP0.5:0.95: {val_seg_metric_all:.4f}\\n\")\n",
    "\n",
    "    # Close progress bars\n",
    "    epoch_pbar.close()\n",
    "    \n",
    "    print(\"\\nTraining completed! Loading best model for testing...\")\n",
    "\n",
    "    # Load the best model for testing\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir, \"best_model.pth\")))\n",
    "    model.eval()\n",
    "\n",
    "    # Test phase with progress bar\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_pbar = tqdm(data_loader_test, desc=\"Testing\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    eval_result_test = evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    # Extract test metrics with error handling\n",
    "    try:\n",
    "        # Try to get both bbox and segm metrics\n",
    "        test_det_metric = eval_result_test.coco_eval['bbox'].stats[1]  # AP50\n",
    "        test_seg_metric = eval_result_test.coco_eval['segm'].stats[1] if 'segm' in eval_result_test.coco_eval else 0.0\n",
    "        \n",
    "        test_det_metric_75 = eval_result_test.coco_eval['bbox'].stats[2]  # AP75\n",
    "        test_seg_metric_75 = eval_result_test.coco_eval['segm'].stats[2] if 'segm' in eval_result_test.coco_eval else 0.0\n",
    "        \n",
    "        test_det_metric_all = eval_result_test.coco_eval['bbox'].stats[0]  # AP0.5:0.95\n",
    "        test_seg_metric_all = eval_result_test.coco_eval['segm'].stats[0] if 'segm' in eval_result_test.coco_eval else 0.0\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Warning: Could not extract some metrics from test evaluation: {e}\")\n",
    "        # Provide default values for missing metrics\n",
    "        if 'bbox' not in eval_result_test.coco_eval:\n",
    "            test_det_metric = test_det_metric_75 = test_det_metric_all = 0.0\n",
    "            print(\"Detection metrics missing\")\n",
    "        if 'segm' not in eval_result_test.coco_eval:\n",
    "            test_seg_metric = test_seg_metric_75 = test_seg_metric_all = 0.0\n",
    "            print(\"Segmentation metrics missing - this may be normal if model doesn't predict masks\")\n",
    "\n",
    "    # Save test results\n",
    "    with open(test_log_file, \"w\") as f:\n",
    "        f.write(f\"Test Results - Detection mAP50: {test_det_metric:.4f}, Segmentation mAP50: {test_seg_metric:.4f}\\n\")\n",
    "        f.write(f\"Test Results - Detection mAP75: {test_det_metric_75:.4f}, Segmentation mAP75: {test_seg_metric_75:.4f}\\n\")\n",
    "        f.write(f\"Test Results - Detection mAP0.5:0.95: {test_det_metric_all:.4f}, Segmentation mAP0.5:0.95: {test_seg_metric_all:.4f}\\n\")\n",
    "\n",
    "    # Save summary metrics\n",
    "    with open(metrics_summary_file, \"w\") as f:\n",
    "        f.write(f\"Best Validation mAP50: {best_eval_metric:.4f}\\n\")\n",
    "        f.write(f\"Final Validation - Detection mAP50: {val_det_metric:.4f}, Segmentation mAP50: {val_seg_metric:.4f}\\n\")\n",
    "        f.write(f\"Final Validation - Detection mAP75: {val_det_metric_75:.4f}, Segmentation mAP75: {val_seg_metric_75:.4f}\\n\")\n",
    "        f.write(f\"Final Validation - Detection mAP0.5:0.95: {val_det_metric_all:.4f}, Segmentation mAP0.5:0.95: {val_seg_metric_all:.4f}\\n\")\n",
    "        f.write(f\"Test Results - Detection mAP50: {test_det_metric:.4f}, Segmentation mAP50: {test_seg_metric:.4f}\\n\")\n",
    "        f.write(f\"Test Results - Detection mAP75: {test_det_metric_75:.4f}, Segmentation mAP75: {test_seg_metric_75:.4f}\\n\")\n",
    "        f.write(f\"Test Results - Detection mAP0.5:0.95: {test_det_metric_all:.4f}, Segmentation mAP0.5:0.95: {test_seg_metric_all:.4f}\\n\")\n",
    "\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Detection mAP50: {test_det_metric:.4f}, Segmentation mAP50: {test_seg_metric:.4f}\")\n",
    "    print(f\"Detection mAP75: {test_det_metric_75:.4f}, Segmentation mAP75: {test_seg_metric_75:.4f}\")\n",
    "    print(f\"Detection mAP0.5:0.95: {test_det_metric_all:.4f}, Segmentation mAP0.5:0.95: {test_seg_metric_all:.4f}\")\n",
    "\n",
    "    # Plot training curves\n",
    "    try:\n",
    "        print(\"Generating training curves...\")\n",
    "        # Loss and learning rate curves\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, num_epochs + 1), train_loss, 'r', label='Train Loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, num_epochs + 1), learning_rates, 'g', label='Learning Rate')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title('Learning Rate')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'loss_lr_curve.png'))\n",
    "\n",
    "        # mAP curves\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(range(1, num_epochs + 1), val_maps, 'b', label='mAP50')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('mAP')\n",
    "        plt.title('Validation mAP50')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(range(1, num_epochs + 1), val_maps_75, 'g', label='mAP75')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('mAP')\n",
    "        plt.title('Validation mAP75')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(range(1, num_epochs + 1), val_maps_all, 'm', label='mAP0.5:0.95')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('mAP')\n",
    "        plt.title('Validation mAP0.5:0.95')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'map_curves.png'))\n",
    "\n",
    "        print(f\"Training curves saved to {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to plot training curves: {e}\")\n",
    "\n",
    "    # Calculate training time\n",
    "    total_time = time.time() - start_time\n",
    "    hours, remainder = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f\"Total training time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "\n",
    "    # Return metrics\n",
    "    val_metrics = (val_det_metric, val_seg_metric, val_det_metric_75, val_seg_metric_75, val_det_metric_all, val_seg_metric_all)\n",
    "    test_metrics = (test_det_metric, test_seg_metric, test_det_metric_75, test_seg_metric_75, test_det_metric_all, test_seg_metric_all)\n",
    "    \n",
    "    return model, best_eval_metric, val_metrics, test_metrics\n",
    "\n",
    "def visualize_prediction(model, image_path, device=None, score_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualize prediction from the Mask R-CNN model on a single image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Mask R-CNN model\n",
    "        image_path: Path to the image file\n",
    "        device: Computation device (CPU/GPU)\n",
    "        score_threshold: Threshold for object detection score\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        device = torch.device('cpu')\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image)[0]\n",
    "    \n",
    "    # Move image to device and perform inference\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor.to(device)])\n",
    "    \n",
    "    # Convert image back to numpy for visualization\n",
    "    image_np = image_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    # Get prediction for the image\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "    scores = prediction[0]['scores'].cpu().numpy()\n",
    "    labels = prediction[0]['labels'].cpu().numpy()\n",
    "    masks = prediction[0]['masks'].cpu().numpy()\n",
    "    \n",
    "    # Filter predictions based on score threshold\n",
    "    valid_indices = scores >= score_threshold\n",
    "    boxes = boxes[valid_indices]\n",
    "    scores = scores[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "    masks = masks[valid_indices]\n",
    "    \n",
    "    # Visualize predictions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Show original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show image with predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image_np)\n",
    "    \n",
    "    # Define colors for different classes\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    \n",
    "    # Draw masks and boxes\n",
    "    for i in range(len(boxes)):\n",
    "        mask = masks[i, 0] > 0.5\n",
    "        label = labels[i]\n",
    "        score = scores[i]\n",
    "        \n",
    "        color = colors[label % len(colors)]\n",
    "        \n",
    "        # Draw mask\n",
    "        masked_image = np.copy(image_np)\n",
    "        masked_image[mask] = masked_image[mask] * 0.7 + color[:3] * 0.3\n",
    "        plt.imshow(masked_image)\n",
    "        \n",
    "        # Draw bounding box\n",
    "        x1, y1, x2, y2 = boxes[i].astype(int)\n",
    "        plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                          fill=False, color=color[:3], linewidth=2))\n",
    "        \n",
    "        # Add label and score\n",
    "        plt.text(x1, y1-5, f\"Class {label}: {score:.2f}\", \n",
    "                 color='white', backgroundcolor=color[:3], fontsize=8)\n",
    "    \n",
    "    plt.title(\"Predictions\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def initialize_dataset(data_root, train_dir, val_dir, test_dir, train_size=100, val_size=50, test_size=50, min_images_per_class=1):\n",
    "    \"\"\"\n",
    "    Initialize dataset directories for training, validation, and testing with size limits.\n",
    "    If dataset directories already exist, they will be deleted and recreated.\n",
    "    \n",
    "    Args:\n",
    "        data_root: Root directory containing images and masks\n",
    "        train_dir: Directory to store training data\n",
    "        val_dir: Directory to store validation data\n",
    "        test_dir: Directory to store test data\n",
    "        train_size: Maximum number of images for training set\n",
    "        val_size: Maximum number of images for validation set\n",
    "        test_size: Maximum number of images for test set\n",
    "        min_images_per_class: Minimum number of images per class in each set\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import random\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Check if dataset directories already exist and delete them if they do\n",
    "    def clean_directory(directory):\n",
    "        if os.path.exists(directory):\n",
    "            print(f\"Removing existing directory: {directory}\")\n",
    "            shutil.rmtree(directory)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Clean and recreate main directories\n",
    "    clean_directory(train_dir)\n",
    "    clean_directory(val_dir)\n",
    "    clean_directory(test_dir)\n",
    "    \n",
    "    # Source directories\n",
    "    src_images_dir = os.path.join(data_root, 'PNGImages')\n",
    "    src_masks_dir = os.path.join(data_root, 'PedMasks')\n",
    "\n",
    "    # Destination directories\n",
    "    dst_train_images_dir = os.path.join(train_dir, 'PNGImages')\n",
    "    dst_train_masks_dir = os.path.join(train_dir, 'PedMasks')\n",
    "    dst_val_images_dir = os.path.join(val_dir, 'PNGImages')\n",
    "    dst_val_masks_dir = os.path.join(val_dir, 'PedMasks')\n",
    "    dst_test_images_dir = os.path.join(test_dir, 'PNGImages')\n",
    "    dst_test_masks_dir = os.path.join(test_dir, 'PedMasks')\n",
    "\n",
    "    # Create destination subdirectories\n",
    "    os.makedirs(dst_train_images_dir, exist_ok=True)\n",
    "    os.makedirs(dst_train_masks_dir, exist_ok=True)\n",
    "    os.makedirs(dst_val_images_dir, exist_ok=True)\n",
    "    os.makedirs(dst_val_masks_dir, exist_ok=True)\n",
    "    os.makedirs(dst_test_images_dir, exist_ok=True)\n",
    "    os.makedirs(dst_test_masks_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Created fresh dataset directories.\")\n",
    "\n",
    "    # Check if source directories exist\n",
    "    if not os.path.exists(src_images_dir):\n",
    "        raise FileNotFoundError(f\"Source images directory not found: {src_images_dir}\")\n",
    "    if not os.path.exists(src_masks_dir):\n",
    "        raise FileNotFoundError(f\"Source masks directory not found: {src_masks_dir}\")\n",
    "\n",
    "    # Get file lists from the source directory\n",
    "    image_files = [f for f in os.listdir(src_images_dir) if f.endswith(('.png', '.jpg'))]\n",
    "    mask_files = [f for f in os.listdir(src_masks_dir) if f.endswith(('.png', '.jpg'))]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images and {len(mask_files)} masks in source directory\")\n",
    "\n",
    "    # Build image-mask pairs and organize by class\n",
    "    image_mask_pairs = []\n",
    "    class_based_pairs = defaultdict(list)\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img_base = os.path.splitext(img_file)[0]\n",
    "        matching_mask = None\n",
    "        img_class = img_file[:3]  # Use first three letters as class identifier\n",
    "\n",
    "        # Look for matching mask in the mask directory\n",
    "        for mask_file in mask_files:\n",
    "            mask_base = os.path.splitext(mask_file)[0]\n",
    "            if img_base == mask_base or mask_file.startswith(img_base):\n",
    "                matching_mask = mask_file\n",
    "                break\n",
    "\n",
    "        if matching_mask:\n",
    "            pair = {\n",
    "                'image': img_file,\n",
    "                'mask': matching_mask,\n",
    "                'class': img_class\n",
    "            }\n",
    "            image_mask_pairs.append(pair)\n",
    "            class_based_pairs[img_class].append(pair)\n",
    "    \n",
    "    # Print class distribution information\n",
    "    print(\"Class distribution in the dataset:\")\n",
    "    for class_name, pairs in sorted(class_based_pairs.items()):\n",
    "        print(f\" - Class {class_name}: {len(pairs)} images\")\n",
    "    \n",
    "    # Get list of all classes\n",
    "    all_classes = list(class_based_pairs.keys())\n",
    "    random.shuffle(all_classes)  # Shuffle to avoid bias\n",
    "    \n",
    "    # Allocate samples per class to meet minimum requirements\n",
    "    train_pairs = []\n",
    "    val_pairs = []\n",
    "    test_pairs = []\n",
    "    remaining_pairs = []\n",
    "    \n",
    "    # First, ensure minimum images per class in each split\n",
    "    for class_name in all_classes:\n",
    "        class_pairs = class_based_pairs[class_name]\n",
    "        random.shuffle(class_pairs)  # Shuffle to randomize selection\n",
    "        \n",
    "        # Add minimum images per class to each split\n",
    "        for i in range(min_images_per_class):\n",
    "            if i < len(class_pairs):\n",
    "                train_pairs.append(class_pairs[i])\n",
    "            \n",
    "        for i in range(min_images_per_class, 2*min_images_per_class):\n",
    "            if i < len(class_pairs):\n",
    "                val_pairs.append(class_pairs[i])\n",
    "            \n",
    "        for i in range(2*min_images_per_class, 3*min_images_per_class):\n",
    "            if i < len(class_pairs):\n",
    "                test_pairs.append(class_pairs[i])\n",
    "        \n",
    "        # Add remaining images to a pool for later allocation\n",
    "        for i in range(3*min_images_per_class, len(class_pairs)):\n",
    "            remaining_pairs.append(class_pairs[i])\n",
    "    \n",
    "    # Shuffle remaining pairs\n",
    "    random.shuffle(remaining_pairs)\n",
    "    \n",
    "    # Fill each set up to the specified size\n",
    "    while len(train_pairs) < train_size and remaining_pairs:\n",
    "        train_pairs.append(remaining_pairs.pop(0))\n",
    "    \n",
    "    while len(val_pairs) < val_size and remaining_pairs:\n",
    "        val_pairs.append(remaining_pairs.pop(0))\n",
    "    \n",
    "    while len(test_pairs) < test_size and remaining_pairs:\n",
    "        test_pairs.append(remaining_pairs.pop(0))\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(train_pairs) < train_size:\n",
    "        print(f\"Warning: Not enough data for training set. Requested {train_size}, but only got {len(train_pairs)}.\")\n",
    "    if len(val_pairs) < val_size:\n",
    "        print(f\"Warning: Not enough data for validation set. Requested {val_size}, but only got {len(val_pairs)}.\")\n",
    "    if len(test_pairs) < test_size:\n",
    "        print(f\"Warning: Not enough data for test set. Requested {test_size}, but only got {len(test_pairs)}.\")\n",
    "    \n",
    "    # Limit each set to exactly the specified size if needed\n",
    "    train_pairs = train_pairs[:train_size]\n",
    "    val_pairs = val_pairs[:val_size]\n",
    "    test_pairs = test_pairs[:test_size]\n",
    "    \n",
    "    # Count final class distribution in each set\n",
    "    train_class_counts = defaultdict(int)\n",
    "    val_class_counts = defaultdict(int)\n",
    "    test_class_counts = defaultdict(int)\n",
    "    \n",
    "    for pair in train_pairs:\n",
    "        train_class_counts[pair['class']] += 1\n",
    "    \n",
    "    for pair in val_pairs:\n",
    "        val_class_counts[pair['class']] += 1\n",
    "    \n",
    "    for pair in test_pairs:\n",
    "        test_class_counts[pair['class']] += 1\n",
    "    \n",
    "    # Print final class distributions\n",
    "    print(\"\\nFinal class distribution in training set:\")\n",
    "    for class_name, count in sorted(train_class_counts.items()):\n",
    "        print(f\" - Class {class_name}: {count} images\")\n",
    "    \n",
    "    print(\"\\nFinal class distribution in validation set:\")\n",
    "    for class_name, count in sorted(val_class_counts.items()):\n",
    "        print(f\" - Class {class_name}: {count} images\")\n",
    "    \n",
    "    print(\"\\nFinal class distribution in test set:\")\n",
    "    for class_name, count in sorted(test_class_counts.items()):\n",
    "        print(f\" - Class {class_name}: {count} images\")\n",
    "    \n",
    "    # Copy files to destination directories\n",
    "    def copy_pairs(pairs, images_dir, masks_dir):\n",
    "        copied = 0\n",
    "        for pair in pairs:\n",
    "            src_img = os.path.join(src_images_dir, pair['image'])\n",
    "            src_mask = os.path.join(src_masks_dir, pair['mask'])\n",
    "            \n",
    "            dst_img = os.path.join(images_dir, pair['image'])\n",
    "            dst_mask = os.path.join(masks_dir, pair['mask'])\n",
    "            \n",
    "            if os.path.exists(src_img) and os.path.exists(src_mask):\n",
    "                shutil.copy(src_img, dst_img)\n",
    "                shutil.copy(src_mask, dst_mask)\n",
    "                copied += 1\n",
    "            else:\n",
    "                print(f\"Warning: Could not find {src_img} or {src_mask}\")\n",
    "        \n",
    "        return copied\n",
    "    \n",
    "    train_copied = copy_pairs(train_pairs, dst_train_images_dir, dst_train_masks_dir)\n",
    "    val_copied = copy_pairs(val_pairs, dst_val_images_dir, dst_val_masks_dir)\n",
    "    test_copied = copy_pairs(test_pairs, dst_test_images_dir, dst_test_masks_dir)\n",
    "    \n",
    "    print(f\"\\nDataset split complete:\")\n",
    "    print(f\" - Training set: {train_copied}/{len(train_pairs)} images copied\")\n",
    "    print(f\" - Validation set: {val_copied}/{len(val_pairs)} images copied\")\n",
    "    print(f\" - Test set: {test_copied}/{len(test_pairs)} images copied\")\n",
    "    \n",
    "    return {\n",
    "        'train_size': train_copied,\n",
    "        'val_size': val_copied,\n",
    "        'test_size': test_copied,\n",
    "        'train_class_distribution': dict(train_class_counts),\n",
    "        'val_class_distribution': dict(val_class_counts),\n",
    "        'test_class_distribution': dict(test_class_counts)\n",
    "    }\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up directories\n",
    "    # Set up directories\n",
    "    data_root = r\".\\SPenn\\SPenn\"  # Path to the original dataset\n",
    "    train_dir = r\"./train\"                  # Path for training data\n",
    "    val_dir = r\"./val\"                      # Path for validation data\n",
    "    test_dir = r\"./test\"                    # Path for test data\n",
    "    output_dir = r\"./maskrcnn_output\"       # Path for saving model and results\n",
    "        \n",
    "    # Initialize dataset and split into train/val/test\n",
    "    stats = initialize_dataset(\n",
    "        data_root = r\".\\SPenn\\SPenn\",  # Path to the original dataset\n",
    "        train_dir = r\"./train\",                  # Path for training data\n",
    "        val_dir = r\"./val\",                      # Path for validation data\n",
    "        test_dir = r\"./test\",                    # Path for test data\n",
    "        train_size=8,    # Limit training set to 80 images\n",
    "        val_size=4,      # Limit validation set to 40 images\n",
    "        test_size=4,     # Limit test set to 40 images\n",
    "        min_images_per_class=2  # Ensure at least 2 images per class in each set\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, best_metric, val_metrics, test_metrics = train_model(\n",
    "        train_dir=train_dir,\n",
    "        val_dir=val_dir,\n",
    "        test_dir=test_dir,\n",
    "        output_dir=output_dir,\n",
    "        num_epochs=1\n",
    "    )\n",
    "    \n",
    "    # Print summary of results\n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"Best validation mAP50: {best_metric:.4f}\")\n",
    "    print(f\"Test Detection mAP50: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Test Segmentation mAP50: {test_metrics[1]:.4f}\")\n",
    "    \n",
    "    # Example of how to use the model for inference on a single image\n",
    "    sample_image_path = \"/path/to/sample/image.jpg\"\n",
    "    if os.path.exists(sample_image_path):\n",
    "        plt = visualize_prediction(model, sample_image_path)\n",
    "        plt.savefig(os.path.join(output_dir, \"sample_prediction.png\"))\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
